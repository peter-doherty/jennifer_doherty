+++ 
date = "2017-09-27"
title = "How to boost your team's output whilst minimizing costs"
slug = "how-to-boost-your-teams-output-whilst-minimizing-costs"
tags = ["professionals","career","Networking"]
categories = ["startup","computation-hub","oil","energy"]
+++
<center>
![](/posts/post_pics/abstract-1231889_640.jpg)
</center>

Traditionally, engineers perform calculations, run simulations and post-process results in a way that is fragmented which restricts team productivity, quality of results and project schedule. Many tools are used to derive modelling inputs (coefficients, material and fluid properties) which in turn are then manually inputted by the engineer into engineering simulation software to perform engineering design analyses. 

The results from commercial software are read back into Excel/Mathcad or other, to post-process results. Afterwards, results are then moved into an MS Word document and emailed to the client (after all quality control checks). 

There are *__many pitfalls__* with this approach resulting in *__reduced productivity__* and *__lost value__* across the cycles of the project. This creates an unpleasant project environment for people in the project chain:

1. Huge amounts of repetitive, manual work take place with data input into the software - this is unnecessary
2. If an error is found at a late stage, a great deal of work has to be checked and re-run
3. New inputs may become available late in the schedule, meaning great efforts are required to adapt

The main areas where barriers exist and that impact productivity and costs are:

*Engineering and business tools*

* Tools of the same type/function are duplicated across your organisation and may not provide the correct results
* Quality control of mathematical formula within a large spreadsheet is a headache
* Activity tracking is not readily available in large spreadsheets/tools i.e. history of what input has changed since the previous revision is ususally missing

![Complex Excel formula](https://www.techonthenet.com/excel/formulas/images/max002.gif)

*Software delivery*

* Some type of [scripting or automation tools are required to automate processes](http://www.abaquspython.com/) which requires your engineers to be knowledgeable in scripting
* If one of the tools/software changes it can disrupt the work-flow path you have invested in, costing you time, money and resources to adjust
* Traditional perpetual license software impair your team's [productivity by restricting the number of users at a given time resulting in lost value](http://blog.condecosoftware.com/saas-vs.-traditional-software-what-do-you-really-need-to-know)
* Computing power is defined to what is available within the organisation meaning analysis run times could suffer
* Deployment of enterprise software with traditional methods is slow, costly and absorbs resources

*Communications*

* Communication in the team is done externally to the project environment via email, phone or face-to-face
* Transfer of results for the client or third party is done externally from the project environment via PDF reports/drawings and perhaps .zip files of tremendous size

*Organisation*

Organisation of team tasks (e.g. assignment, monitoring of tasks) is done outside of the project environment making it difficult to manage upon handover of the Project Manager.

*Data acquisition*

Assets that are fitted with sensors and data acquisition hardware capture data in real-time, 24/7/365 in complex environments. Unfortunately, the industry has generally struggled to efficiently create a channel for data to flow from the asset back to the office to analyse how the asset is behaving and when a failure will occur.

*Contracts*

Contracts such as Engineering, Procurement, Construction and Installation (EPCI) involve enormous amounts of paperwork. It can be a very challenging process to disseminate project documents, company specifications and other information to contractors. Thorough tracking, monitoring and control are necessary from all parties involved (operators, contractors, sub-contractors and classification societies) to ensure that the correct and latest information is transmitted for all cycles of the project. 

Nowadays, this is undertaken with several document control systems and Excel-based tools which generally do not link well with each other. Moreover, contractors may have different systems of their own for document handling meaning, more efforts are required to share information within their teams. This results in an intricate process that could benefit from streamlining.

**Centralised, collaborative digital hub**

A *__centralised, digital workspace__* where people can work together in real-time, access engineering tools, software and documents that are managed online is an ideal solution to solve the above challenges. With an online hub, this can be achieved in at least three phases - this is our *__roadmap__* at Computation Hub:

1. [Transforming existing tools](/services/digital-transformation) into web applications
2. Developing [custom software](/services/pipelay-cablelay-installation) to solve current or emerging issues
3. Using open source finite element and computational fluid dynamic engines to develop general purpose applications

![Centralised workspace for your teams](digital_solutions_hub_clean.png "Centralised workspace for your teams")

The [five biggest companies in the world by market capitalisation](http://www.bbc.com/news/business-39875417) are Apple, Google (Alphabet), Microsoft, Amazon and Facebook. They are all are heavy users of Machine Learning and Cloud computing to understand trends and patterns in the massive amounts of data that they collect. This empowers their business by giving them valuable insights/predictions for their operations, allowing them to be cost-effective and run on time. 

Typically, the Oil & Gas industry have been quite strong in developing and innovating their simulation models for all aspects of their business. What can be done nowadays is tune those models with their data lakes to achieve those valuable insights/predictions for their operations just like the aforementioned technology companies do. This can be done in the following ways:

1. Deploy data-driven, intelligent software across assets and operations in small, fast and scalable phases
2. Adopt the methodology of the aforementioned data-driven companies
3. Help companies leverage value from the pools of their data already available

The above three steps form part of [our purpose at Computation Hub](/team).

It has been widely reported that companies are thinking about using an online/digital platform, available in several formats. There are *__two options__* discussed here. *__Option one__* is to develop a closed, internal platform and manage everything privately. *__Option two__* would be to use an open platform where all companies involved in a project can work collaboratively. There are other [options in between what I outlined](http://www.episerver.com/learn/resources/blog/fred-bals/pizza-as-a-service/). 

There are several reasons why both these options can be challenging and require "buy-in" from many departments and stakeholders within the organisation. Department managers and IT Services Directors within organisations will typically evaluate the following topics (but not limited to):

* Applications (e.g. browser-based, native)
* Cybersecurity
* Data (ownership, privacy, theft, sharing etc.)
* Runtime
* Server maintenance
* Storage
* Software updates
* Networking
* Operating system
* User experience
* Visualisation

Moreover, responsibilities such as IT personnel, customisation, implementation, licensing are the responsibility of the owner. For an open platform, those responsibilities are placed with the vendor. Occasionally, software upgrades need to be considered - does your department have the resources to take on an upgrade? Are the benefits versus the effort worth it? How is that decided? With an open platform, again, these decisions are taken care for the user.

With a digital hub like the one outlined, it may lead to the misconception that it is a sort of "locked-in ecosystem" - this is not the case. In the event that a single company wants to discontinue the service provided, they can download their information and data in a standardised output format.

**Intelligent software**

15-20 years ago, if you wished to watch the latest movie one would connect a VCR or DVD player to your television, drive to the local store, choose your movie and then return home to watch it. As a comparison, in terms of software, this is what how our industry operates. 

Services like Netflix allows you to watch movies anywhere in the world, on any web-enabled device for a single monthly subscription. Essentially, this is accessed via a Software-as-a-Service (SaaS) delivery model. This is where our industry should be heading to reduce [software Total Cost of Ownership (TCO)](http://www.investopedia.com/terms/t/totalcostofownership.asp).

Lately, Oil &amp; Energy industry is moving towards adopting digital technologies for their business to embrace lower oil prices. Research has shown that:
1. 56% of Oil &amp; Gas companies [plan to use the Cloud in the next 3-5 years](https://www.accenture.com/us-en/insight-2016-upstream-oil-gas-digital-trends-survey)
2. 86-90% of companies [believe analytical, mobile and Internet of Things (IoT) capabilities will add value to their business](http://www.forbes.com/sites/bernardmarr/2015/05/26/big-data-in-big-oil-how-shell-uses-analyticsto-drive-business-success/6cff71274b97)

Moreover, digital technologies are being adopted by the renewable energy industry to forecast wind-farm performance and asset reliability (for example). 

Software and engineering tools should be "web first" meaning they should be accessible in the web browser, at a minimum. Old tools can be enhanced by converting them to web applications and adding intelligence within them to create value from your data. This is possible because open source libraries for data visualisation and Big Data analytics are more widely available and robustly supported. As a whole, these applications can then be delivered as a service (i.e. SaaS) which is in-line with budget holders' value-driven initiatives.

The benefits of this approach are:

* Flexible, pay-as-you-go software that reduces your TCO
* Connection to on-demand cloud computing which allows you to analyse data faster to allow quicker decision-making
* Prediction of scenarios with Machine Learning that allow you to avoid expensive downtime for equipment repairs or unexpected failures
* Higher levels of productivity by streamlining processes which reduce your operating expenditures
* Increased safety by adopting predictive analytics to analyse data recorded via intelligent online applications
* Reduced risks by making tasks autonomous which means elimination of transporting people offshore
* Rapid upgrade or downgrade the number of users allowing you to manage costs easily

The issues surrounding contracts and sharing of information could be solved with [blockchain technologies](https://www.youtube.com/watch?v=RplnSVTzvnU). This would be quite beneficial for the industry and with a digital hub to support it, big time savings can be achieved.

**To sum up**

There are certain, old traditional standards and ways that need to remain in place, yet there are still spaces for areas to succeed with newer technology. Key issues highlighted by industry - [data privacy](https://www.twobirds.com/en/news/articles/2017/global/data-ownership-in-the-context-of-the-european-data-economy), disaster recovery and cybersecurity - are not insurmountable, yet will require patience and perseverance to be fully accepted in the industry. 

At Computation Hub, we believe that providing an open digital hub, with flexible, SaaS solutions to unlock significant cost and time savings can benefit organisations for the better. In further blog posts, we will explore several ways in which this could be achieved.